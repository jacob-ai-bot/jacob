import { dedent } from "ts-dedent";
import {
  type Model,
  sendGptRequest,
  sendGptRequestWithSchema,
} from "~/server/openai/request";
import { db } from "~/server/db/db";
import { parseTemplate } from "../utils";
import {
  type ContextItem,
  getOrCreateCodebaseContext,
} from "../utils/codebaseContext";
import { traverseCodebase } from "../analyze/traverse";
import { type StandardizedPath, standardizePath } from "../utils/files";
import { z } from "zod";
import { ResearchAgentActionType } from "~/types";

export interface Research {
  todoId: number;
  issueId: number;
  type: ResearchAgentActionType;
  question: string;
  answer: string;
}

interface ResearchIssueParams {
  githubIssue: string;
  todoId: number;
  issueId: number;
  rootDir: string;
  projectId: number;
  maxLoops?: number;
  model?: Model;
}

const ResearchSchema = z.object({
  questions: z
    .array(
      z.object({
        type: z.nativeEnum(ResearchAgentActionType),
        question: z.string(),
      }),
    )
    .max(10),
});
type ResearchSchema = z.infer<typeof ResearchSchema>;

export const researchIssue = async function ({
  githubIssue,
  todoId,
  issueId,
  rootDir,
  projectId,
  maxLoops = 1, // don't loop for now, we'll just do one pass
  model = "claude-3-5-sonnet-20241022",
}: ResearchIssueParams): Promise<Research[]> {
  console.log("Researching issue...");
  // First get the context for the full codebase
  const allFiles = traverseCodebase(rootDir);
  const query = `Based on the GitHub issue, your job is to identify the most important files to review in this codebase.\n
  Here is the issue <issue>${githubIssue}</issue> \n
  Based on the GitHub issue, what are the 5 most relevant files to review for understanding and researching this GitHub issue? Only select the minimum number of files necessary to answer the research question, up to a maximum of 5 files. Order them from most relevant to least relevant. Focus only on reviewing existing files, not files that need to be created or modified.`;
  const relevantFiles = await selectRelevantFiles(
    query,
    undefined,
    allFiles,
    100,
  );

  const codebaseContext = await getOrCreateCodebaseContext(
    projectId,
    rootDir,
    relevantFiles.map((file) => standardizePath(file)) ?? [],
  );
  // For now, change the sourcemap to be a list of all the files from the context and overview of each file
  const sourceMap = codebaseContext
    .map((file) => `${file.file} - ${file.overview}`)
    .join("\n");

  const researchTemplateParams = {
    githubIssue,
    sourceMap,
  };
  const systemPrompt = parseTemplate(
    "research",
    "research_issue",
    "system",
    researchTemplateParams,
  );
  let userPrompt = parseTemplate(
    "research",
    "research_issue",
    "user",
    researchTemplateParams,
  );

  const gatheredInformation: Research[] = [];
  let loops = 0;

  while (loops < maxLoops) {
    loops++;
    try {
      const response = (await sendGptRequestWithSchema(
        userPrompt,
        systemPrompt,
        ResearchSchema,
        0.3,
        undefined,
        3,
        model,
      )) as ResearchSchema;

      for (const question of response.questions) {
        const functionResponse = await callFunction(
          question.type as ResearchAgentActionType,
          question.question,
          githubIssue,
          sourceMap,
          codebaseContext,
        );
        const research: Research = {
          todoId,
          issueId,
          type: question.type,
          question: question.question,
          answer: functionResponse,
        };
        gatheredInformation.push(research);
        await db.research.create(research);
      }
      if (response.questions.length === 0) {
        break;
      }
      userPrompt = dedent`
      ### Gathered Information:
      ${gatheredInformation.map((r) => `### ${r.type} \n\n#### Question: ${r.question} \n\n${r.answer}`).join("\n")}
      ### Missing Information:
      Reflect on the gathered information and specify what is still needed to fully address the issue and why it is needed.
      ### Plan Information Gathering:
      Decide on the best action to obtain each missing piece of information (ResearchCodebase, ResearchInternet, AskProjectOwner).
      Choose the correct types and formulate very specific, detailed queries to gather all of the missing information effectively.
      ### Important:
      If you have all the necessary information to proceed with the task, return an object that matches the ResearchSchema schema with an empty array of questions. Otherwise, generate up to 10 additional questions to gather more information.
    `;
    } catch (error) {
      console.error("Error in research loop:", error);
      break;
    }
  }

  if (loops >= maxLoops) {
    console.log("Max loops reached, exiting loop.");
  }
  return gatheredInformation;
};

async function callFunction(
  functionName: ResearchAgentActionType,
  question: string,
  githubIssue: string,
  sourceMap: string,
  codebaseContext: ContextItem[],
): Promise<string> {
  switch (functionName) {
    case ResearchAgentActionType.ResearchCodebase:
      return await researchCodebase(
        question,
        githubIssue,
        sourceMap,
        codebaseContext,
      );
    case ResearchAgentActionType.ResearchInternet:
      return await researchInternet(question);
    case ResearchAgentActionType.AskProjectOwner:
      // just return a blank answer, the user will answer it later
      return "";
    default:
      return "Function not found.";
  }
}
export async function researchCodebase(
  query: string,
  githubIssue: string,
  sourceMap: string,
  codebaseContext: ContextItem[],
): Promise<string> {
  const allFiles = codebaseContext.map((file) => standardizePath(file.file));

  let relevantFiles: string[];
  if (allFiles.length <= 50) {
    relevantFiles = allFiles;
  } else {
    relevantFiles = await selectRelevantFiles(query, codebaseContext);
  }
  // get the context for all of the relevant files
  const relevantContext = codebaseContext.filter((file) =>
    relevantFiles.includes(file.file),
  );

  const codebase = JSON.stringify(relevantContext, null, 2);

  const codeResearchTemplateParams = {
    codebase,
    sourceMap,
    githubIssue,
    query,
  };

  const codeResearchSystemPrompt = parseTemplate(
    "research",
    "research_codebase",
    "system",
    codeResearchTemplateParams,
  );
  const codeResearchUserPrompt = parseTemplate(
    "research",
    "research_codebase",
    "user",
    codeResearchTemplateParams,
  );
  let result: string | null = "";

  // First try to send the request to the claude model. If that fails because the codebase is too large, call gemini.
  try {
    result = await sendGptRequest(
      codeResearchUserPrompt,
      codeResearchSystemPrompt,
      0.3,
      undefined,
      0, // no retries, so we can quickly failover to gemini
      60000,
      null,
      "claude-3-5-sonnet-20241022",
    );
  } catch (error) {
    result = await sendGptRequest(
      codeResearchUserPrompt,
      codeResearchSystemPrompt,
      0.3,
      undefined,
      2,
      60000,
      null,
      "gemini-1.5-pro-latest",
    );
  }

  return result ?? "No response from the AI model.";
}
// Define the schema for the response
const RelevantFilesSchema = z.object({
  files: z.array(z.string()),
});
type RelevantFiles = z.infer<typeof RelevantFilesSchema>;

export async function selectRelevantFiles(
  query: string,
  codebaseContext?: ContextItem[],
  allFiles?: StandardizedPath[],
  numFiles = 50,
): Promise<StandardizedPath[]> {
  if (!codebaseContext && !allFiles) {
    throw new Error("Either codebaseContext or allFiles must be provided.");
  }
  if (allFiles && allFiles.length <= numFiles) {
    return allFiles;
  }
  const selectFilesTemplateParams = {
    query,
    allFiles: allFiles
      ? allFiles.join("\n")
      : codebaseContext
          ?.map(
            (file) =>
              `${file.file} - ${file.text}\n Diagram: ${file.diagram ?? ""}`,
          )
          .join("\n") ?? "",
    numFiles: numFiles.toString(),
  };
  if (!allFiles) {
    allFiles = codebaseContext?.map((file) => standardizePath(file.file));
  }

  const selectFilesSystemPrompt = parseTemplate(
    "research",
    "select_files",
    "system",
    selectFilesTemplateParams,
  );
  const selectFilesUserPrompt = parseTemplate(
    "research",
    "select_files",
    "user",
    selectFilesTemplateParams,
  );

  try {
    const response = (await sendGptRequestWithSchema(
      selectFilesUserPrompt,
      selectFilesSystemPrompt,
      RelevantFilesSchema,
      0.3,
      undefined,
      3,
      "claude-3-5-sonnet-20241022",
    )) as RelevantFiles;

    if (!response.files) {
      throw new Error("No files found in response");
    }

    // convert relevant files to standard paths
    const relevantFiles = response.files
      .map(standardizePath)
      .filter((p) => p?.length);

    // Filter the relevant files to ensure they exist in allFiles
    const filteredRelevantFiles = relevantFiles.filter((file) =>
      allFiles?.some((setFile) => setFile === file),
    );

    console.log("Top 10 relevant files:", filteredRelevantFiles.slice(0, 10));
    console.log(
      `Bottom ${numFiles - 10} relevant files:`,
      filteredRelevantFiles.slice(-10),
    );
    // remove duplicates and return the top numFiles
    return Array.from(new Set(filteredRelevantFiles)).slice(0, numFiles);
  } catch (error) {
    console.error("Error selecting relevant files:", error);
    return [];
  }
}

export async function researchInternet(query: string): Promise<string> {
  const internetResearchTemplateParams = {
    query,
  };

  const internetResearchSystemPrompt = parseTemplate(
    "research",
    "research_internet",
    "system",
    internetResearchTemplateParams,
  );
  const internetResearchUserPrompt = parseTemplate(
    "research",
    "research_internet",
    "user",
    internetResearchTemplateParams,
  );

  const result = await sendGptRequest(
    internetResearchUserPrompt,
    internetResearchSystemPrompt,
    0.3,
    undefined,
    2,
    60000,
    null,
    "llama-3.1-sonar-large-128k-online",
  );

  return result ?? "No response from the AI model.";
}
